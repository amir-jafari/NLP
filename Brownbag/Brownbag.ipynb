{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amir-jafari/NLP/blob/master/Brownbag/Brownbag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle']\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle', ',', 'there', 'lived', 'a', '{', 'animal', '}', '.', 'This', '{', 'animal', '}', 'liked', 'to', 'eat', '{', 'food', '}', ',', 'but', 'the', 'jungle', 'had', 'very', 'little', '{', 'food', '}', 'to', 'offer', '.', 'One', 'day', ',', 'an', 'explorer', 'found', 'the', '{', 'animal', '}', 'and', 'discovered', 'it', 'liked', '{', 'food', '}', '.', 'The', 'explorer', 'took', 'the', '{', 'animal', '}', 'back', 'to', '{', 'city', '}', ',', 'where', 'it', 'could', 'eat', 'as', 'much', '{', 'food', '}', 'as', 'it', 'wanted', '.', 'However', ',', 'the', '{', 'animal', '}', 'became', 'homesick', ',', 'so', 'the', 'explorer', 'brought', 'it', 'back', 'to', 'the', 'jungle', ',', 'leaving', 'a', 'large', 'supply', 'of', '{', 'food', '}', '.', 'The', 'End']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import  nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "Story = \"\"\"\n",
    "Once upon a time, deep in an ancient jungle,there lived a {animal}. This {animal} liked to eat {food}, but the jungle had \n",
    "very little {food} to offer. One day, an explorer found the {animal} and discovered it liked {food}. The explorer took the\n",
    "{animal} back to {city}, where it could eat as much {food} as it wanted. However, the {animal} became homesick, so the\n",
    "explorer brought it back to the jungle, leaving a large supply of {food}.\n",
    "\n",
    "The End\n",
    "\"\"\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(Story)\n",
    "print(type(tokens))\n",
    "print(tokens[:10])\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(120*'-')\n",
    "print( [lancaster.stem(t) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "tokens = word_tokenize(raw)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwprds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "0.5909126139346464\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "print(content_fraction(nltk.corpus.brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4), ('faster', 3), ('harry', 2), (',', 2)]\n",
      "0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"The faster Harry got to the store, the faster Harry \" \\\n",
    "           \"the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words.most_common(4))\n",
    "\n",
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "tf = times_harry_appears / num_unique_words\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logsitic Regression\n",
    "## Example 1:\n",
    "\n",
    "This dataset consist of text and labels. We are trying to use NLP techniques to classify each response. **Our goal** is to correctly model the texts into its label.\n",
    "\n",
    "Hint: \n",
    "\n",
    "1. You need to read the text responses and perform preprocessing on it, such as normalization, legitimation, cleaning, stopwords removal and POS tagging.\n",
    "\n",
    "2. Use any tokenization method to convert each response into meaningful vectors.\n",
    "\n",
    "3. Apply Naive bayes and look at appropriate evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  80.5\n",
      "\n",
      "Naive Bayes Confusion Matrix -> \n",
      "  [[99 12]\n",
      " [27 62]]\n",
      "\n",
      "Naive Bayes Cohen Kappa Score ->  0.5985176034589252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "np.random.seed(500)\n",
    "\n",
    "Data = pd.read_csv(\"https://raw.githubusercontent.com/amir-jafari/NLP/master/Brownbag/data.csv\", encoding='latin-1').head(1000)\n",
    "Data['text'].dropna(inplace=True)\n",
    "Data['text'] = [entry.lower() for entry in Data['text']]\n",
    "Data['text'] = [word_tokenize(entry) for entry in Data['text']]\n",
    "\n",
    "tag_mapping = defaultdict(lambda: wn.NOUN)\n",
    "tag_mapping['J'] = wn.ADJ\n",
    "tag_mapping['V'] = wn.VERB\n",
    "tag_mapping['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(Data['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_mapping[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Data.loc[index, 'text_final'] = str(Final_words)\n",
    "    \n",
    "X_Train, X_Test, Y_Train, Y_Test = model_selection.train_test_split(Data['text_final'], Data['label'], test_size=0.2)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Y_Train = Encoder.fit_transform(Y_Train)\n",
    "Y_Test = Encoder.fit_transform(Y_Test)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=2000)\n",
    "Tfidf_vect.fit(Data['text_final'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_Train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_Test)\n",
    "\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Y_Train)\n",
    "\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(Y_Test, predictions_NB) * 100)\n",
    "print()\n",
    "print(\"Naive Bayes Confusion Matrix -> \\n \", confusion_matrix(Y_Test, predictions_NB))\n",
    "print()\n",
    "print(\"Naive Bayes Cohen Kappa Score -> \", cohen_kappa_score(Y_Test, predictions_NB))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'strong', 'king', 'boy', 'wise', 'young', 'woman', 'queen', 'girl', 'prince', 'pretty', 'princess', 'man'}\n",
      "king strong man\n",
      "queen wise woman\n",
      "boy young man\n",
      "girl young woman\n",
      "prince young king\n",
      "princess young queen\n",
      "man strong\n",
      "woman pretty\n",
      "prince boy king\n",
      "princess girl queen\n",
      "    input   label\n",
      "0    king  strong\n",
      "1    king     man\n",
      "2  strong    king\n",
      "3  strong     man\n",
      "4     man    king\n",
      "5     man  strong\n",
      "6   queen    wise\n",
      "7   queen   woman\n",
      "8    wise   queen\n",
      "9    wise   woman\n",
      "(52, 2)\n",
      "Epoch 0 | Loss 0.07768\n",
      "Epoch 1000 | Loss 0.06208\n",
      "Epoch 2000 | Loss 0.06194\n",
      "Epoch 3000 | Loss 0.06187\n",
      "Epoch 4000 | Loss 0.06183\n",
      "Epoch 5000 | Loss 0.06181\n",
      "Epoch 6000 | Loss 0.06180\n",
      "Epoch 7000 | Loss 0.06179\n",
      "Epoch 8000 | Loss 0.06178\n",
      "Epoch 9000 | Loss 0.06177\n",
      "Epoch 10000 | Loss 0.06177\n",
      "Epoch 11000 | Loss 0.06177\n",
      "Epoch 12000 | Loss 0.06176\n",
      "Epoch 13000 | Loss 0.06176\n",
      "Epoch 14000 | Loss 0.06175\n",
      "Epoch 15000 | Loss 0.06175\n",
      "Epoch 16000 | Loss 0.06175\n",
      "Epoch 17000 | Loss 0.06175\n",
      "Epoch 18000 | Loss 0.06175\n",
      "Epoch 19000 | Loss 0.06175\n",
      "[[-13.893628    -9.248417  ]\n",
      " [  2.2788942   -0.37978154]\n",
      " [  1.8531835   -0.6350897 ]\n",
      " [-30.954584    -9.849318  ]\n",
      " [  1.9165531   -0.4743043 ]\n",
      " [ -5.1588078   14.6768    ]\n",
      " [  2.0450623    0.07201156]\n",
      " [ -5.7736573   -1.2883117 ]\n",
      " [  6.0080447   -5.792962  ]\n",
      " [-33.223152   -18.171959  ]\n",
      " [ -9.037369    21.07759   ]\n",
      " [  5.889415    -5.320923  ]]\n",
      "        word         x1         x2\n",
      "0     strong -13.893628  -9.248417\n",
      "1       king   2.278894  -0.379782\n",
      "2        boy   1.853184  -0.635090\n",
      "3       wise -30.954584  -9.849318\n",
      "4      young   1.916553  -0.474304\n",
      "5      woman  -5.158808  14.676800\n",
      "6      queen   2.045062   0.072012\n",
      "7       girl  -5.773657  -1.288312\n",
      "8     prince   6.008045  -5.792962\n",
      "9     pretty -33.223152 -18.171959\n",
      "10  princess  -9.037369  21.077591\n",
      "11       man   5.889415  -5.320923\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy\n",
    "# -------------------------------------------------------------------------------------\n",
    "LR = 1e-2\n",
    "N_EPOCHS =20000\n",
    "PRINT_LOSS_EVERY = 1000\n",
    "EMBEDDING_DIM = 2\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "corpus = ['king is a strong man',\n",
    "          'queen is a wise woman',\n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong',\n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen']\n",
    "# -------------------------------------------------------------------------------------\n",
    "def remove_stop_words(corpus):\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(\" \".join(tmp))\n",
    "\n",
    "    return results\n",
    "# -------------------------------------------------------------------------------------\n",
    "corpus = remove_stop_words(corpus)\n",
    "# -------------------------------------------------------------------------------------\n",
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "words = set(words)\n",
    "print(words)\n",
    "# -------------------------------------------------------------------------------------\n",
    "word2int = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "\n",
    "sentences = []\n",
    "for sentence in corpus:\n",
    "    sentences.append(sentence.split())\n",
    "# -------------------------------------------------------------------------------------\n",
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0): min(idx + WINDOW_SIZE, len(sentence)) + 1]:\n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "# -------------------------------------------------------------------------------------\n",
    "for text in corpus:\n",
    "    print(text)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])\n",
    "print(df.head(10))\n",
    "print(df.shape)\n",
    "# -------------------------------------------------------------------------------------\n",
    "ONE_HOT_DIM = len(words)\n",
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding\n",
    "# -------------------------------------------------------------------------------------\n",
    "X = []\n",
    "Y = []\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "# -------------------------------------------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(12, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 12)\n",
    "        self.act1 = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        out_em = self.linear1(x)\n",
    "        output = self.linear2(out_em)\n",
    "        output = self.act1(output)\n",
    "        return out_em, output\n",
    "p = torch.Tensor(X_train)\n",
    "p.requires_grad = True\n",
    "t = torch.Tensor(Y_train)\n",
    "# %% -------------------------------------- Training Prep --------------------------------------------------------------\n",
    "model = MLP(EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "# %% -------------------------------------- Training Loop --------------------------------------------------------------\n",
    "for epoch in range(N_EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    _, t_pred = model(p)\n",
    "    loss = criterion(t, t_pred)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % PRINT_LOSS_EVERY == 0:\n",
    "        print(\"Epoch {} | Loss {:.5f}\".format(epoch, loss.item()))\n",
    "\n",
    "vectors = model.linear1._parameters['weight'].cpu().detach().numpy().transpose()\n",
    "print(vectors)\n",
    "# -------------------------------------------------------------------------------------\n",
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = list(words)\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "print(w2v_df)\n",
    "# -------------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1, x2))\n",
    "\n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    "\n",
    "plt.xlim(x_axis_min, x_axis_max)\n",
    "plt.ylim(y_axis_min, y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sentiment Analysis With LSTM\n",
    "\n",
    "IWe will be building a machine learning model to detect sentiment using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. \n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "\n",
    "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  include_lengths = True)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "The next stage is building the model that we'll eventually train and evaluate. \n",
    "\n",
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
    "\n",
    "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
    "\n",
    "![](https://github.com/amir-jafari/NLP/blob/master/Brownbag/sentiment7.png?raw=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0447,  0.0882, -0.1086,  ..., -0.8002,  1.0247, -0.3785],\n",
      "        ...,\n",
      "        [ 1.3049,  1.6111,  1.6621,  ..., -0.9554, -2.8356, -0.6435],\n",
      "        [-1.1801, -0.7227, -1.8904,  ...,  0.9995,  0.2652,  0.5127],\n",
      "        [ 0.5679, -0.5493, -0.2391,  ...,  0.7662, -0.8018, -0.4563]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll set up the training and then train the model.\n",
    "\n",
    "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "## pipline\n",
    "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9718774557113647}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting to listen to the Brownbag session my whole life.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9839903712272644}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I've been waiting to listen to this terrible Brownbag session my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9992126822471619}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am not sure what I am doing but seems I am doing a heck of job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9985759258270264}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"How are you doing?.I am not too bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "Well start by tackling a more challenging task where we need to classify texts that havent been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the **zero-shot-classification** pipeline is very powerful: \n",
    "\n",
    "it allows you to specify which labels to use for the classification, so you dont have to rely on the labels of the pretrained model. Youve already seen how the model can classify a sentence as positive or negative using those two labels  but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71548e4300df4472861268149e2cf38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ac37243a56443a847334a20a95e29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852296d0d392406897b7567cddeda892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16e57b52c904557b875ff7c90db896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a030c979554f9f8460603e7d64e7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795dcddc331b481992700932f0626aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445967435836792, 0.1119757890701294, 0.04342745989561081]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This is a course about the Transformers library\", candidate_labels=[\"education\", \"politics\", \"business\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This brownbag session is about the machine learning and artifical inteligence',\n",
       " 'labels': ['data science', 'education', 'business', 'politics'],\n",
       " 'scores': [0.6876233220100403,\n",
       "  0.15481111407279968,\n",
       "  0.12108190357685089,\n",
       "  0.03648364171385765]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This brownbag session is about the machine learning and artifical inteligence\", \n",
    "           candidate_labels=[\"education\", \"politics\", \"business\", \"data science\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "Now lets see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so its normal if you dont get the same results as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016434999e3546e796ef39828c1ad301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ee23bd601e4a7fa50d4ea4e5b3af96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015a0436fcd048f985dc3b3535c66351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f6806fc56e422bb58bc069ee7044c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d2ae67504f48f0ace054cec2779d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop your own and share with others your skill, knowledge and skill in business, networking, marketing and technology. In order to practice this, your skills and insight required will vary.\\n\\nLearn more'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I am tired of listening to this brownbag session about natural language processing. I am tired of thinking about the question of why we don't have our minds trained on the fact that language learning cannot be developed in the classroom.\\n\\nFor me the answer is simple. We are not machines  nor are we talking about computers. Our brains have evolved and we have evolved to be computerized. If we are to get our brain to develop natural language processing or understand the language in a more natural way\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"I am tired of listening to this brownbag session about natural language processing.\",num_return_sequences = 1, max_length  = 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using any model from the Hub in a pipeline\n",
    "\n",
    "The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task  say, text generation.\n",
    "\n",
    "Go to the [Model Hub](https://huggingface.co/course/chapter1/3?fw=pt) and click on the corresponding tag on the left to display only the supported models for that task. \n",
    "\n",
    "Lets try the distilgpt2 model! Heres how to load it in the same pipeline as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c078e39eeb845ca8dbff8e687051505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249809f0e7f241af84419d8fce77fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d12011eef5497bacf4e1c8a45cbc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ce24b90aae4268b0eeb5c62b6695ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e40478fde3f4957aef79cda5a0076c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to make some use of both the new and the new techniques you want to use. By reading from these'},\n",
       " {'generated_text': 'In this course, we will teach you how to become a well-meaning individual who works for the community as a citizen.\\n\\n\\n\\n\\n'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this session, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask filling\n",
    "\n",
    "The next pipeline youll try is fill-mask. The idea of this task is to fill in the blanks in a given text:\n",
    "\n",
    "\n",
    "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special **\\<mask\\>** word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so its always good to verify the proper mask word when exploring other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32307dfd5f249569f9739ffe07dd144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff002828c154c2e901a1e118f8b5083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb3dc57080b4c39a03e314097507ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f136c2a144398bf921abac96afa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74401456c6c944c48122927a4cfdff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This session will teach you all about mathematical models.',\n",
       "  'score': 0.1348658800125122,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This session will teach you all about computational models.',\n",
       "  'score': 0.040120597928762436,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This session will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I lost my life.',\n",
       "  'score': 0.18934619426727295,\n",
       "  'token': 685,\n",
       "  'token_str': ' lost'},\n",
       " {'sequence': 'I love my life.',\n",
       "  'score': 0.11497117578983307,\n",
       "  'token': 657,\n",
       "  'token_str': ' love'},\n",
       " {'sequence': 'I changed my life.',\n",
       "  'score': 0.09439051151275635,\n",
       "  'token': 1714,\n",
       "  'token_str': ' changed'},\n",
       " {'sequence': 'I hate my life.',\n",
       "  'score': 0.07370663434267044,\n",
       "  'token': 4157,\n",
       "  'token_str': ' hate'},\n",
       " {'sequence': 'I loved my life.',\n",
       "  'score': 0.041104670614004135,\n",
       "  'token': 2638,\n",
       "  'token_str': ' loved'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"I <mask> my life.\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. Lets look at an example\n",
    "\n",
    "We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99919975,\n",
       "  'word': 'Amir',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99658424,\n",
       "  'word': 'CL',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9961011,\n",
       "  'word': 'District of Columbia',\n",
       "  'start': 36,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Amir and I work at CL in District of Columbia office.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering\n",
    "\n",
    "The question-answering pipeline answers questions using information from a given context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.43504536151885986,\n",
       " 'start': 30,\n",
       " 'end': 66,\n",
       " 'answer': 'CL in in District of Columbia office'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Amir and I work at CL in in District of Columbia office.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6118114590644836,\n",
       " 'start': 671,\n",
       " 'end': 683,\n",
       " 'answer': '709 billion'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    #question=\"Where is the capital of France?\",\n",
    "    #question=\"What is population Paris?\",\n",
    "    question=\"What is GDP of Paris?\",\n",
    "    context=\"\"\"\n",
    "    Paris (French pronunciation: (About this soundlisten)) is the capital and most \n",
    "    populous city of France, with an estimated population of 2,175,601 residents as of 2018,\n",
    "    in an area of more than 105 square kilometres (41 square miles).[4] Since the 17th century,\n",
    "    Paris has been one of Europe's major centres of finance, diplomacy, commerce,\n",
    "    fashion, gastronomy, science, and arts. The City of Paris is the centre and seat\n",
    "    of government of the region and province of le-de-France, or Paris Region, \n",
    "    which has an estimated population of 12,174,880, or about 18 percent of the population\n",
    "    of France as of 2017.[5] The Paris Region had a GDP of 709 billion ($808 billion)\n",
    "    in 2017.[6] According to the Economist Intelligence Unit Worldwide Cost of Living Survey\n",
    "    in 2018, Paris was the second most expensive city in the world, after Singapore and ahead \n",
    "    of Zrich, Hong Kong, Oslo, and Geneva.[7] Another source ranked Paris as most expensive,\n",
    "    on a par with Singapore and Hong Kong, in 2018.[8][9]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Heres an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8f92645c4c441a829008a7d4b8b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f99df95e9d4bbf96b082be2e86cd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7839837d484133bf87350ca4b75996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b68b50d3044b8f89e4655d641abfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331961935b0145318a171a4e6a6022ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\",max_length= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with a tokenizer\n",
    "\n",
    "Once we have the tokenizer, we can directly pass our sentences to it and well get back a dictionary thats ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
    "\n",
    "You can use Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow orsome models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. Its effectively a tensor; other ML frameworks tensors behave similarly, and are usually as simple to instantiate as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877edc6cdf474f1891bb3b6c686996c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d63ab91bcd4add8865a81ebe9e087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e8e25d56be4ec0be0b5847e708fe4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be840d9a8a41c6abadd424351e3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037,  2023,  2829,\n",
      "         16078,  5219,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'this', 'brown', '##bag', 'session', '.']\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = \"I've been waiting for a this brownbag session.\"\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(tokenizer.tokenize(raw_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, input_ids and attention_mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Head\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we wont actually use the AutoModel class, but AutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([[-0.1698,  0.1336]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.4247, 0.5753]], grad_fn=<SoftmaxBackward>)\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "raw_inputs = \"I've been waiting for a this brownbag session.\"\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "The tokenization process is done by the tokenize method of the tokenizer.The conversion to input IDs is handled by the convert_tokens_to_ids tokenizer method.\n",
    "\n",
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode method as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "print(num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "metric= load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
