{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amir-jafari/NLP/blob/master/Brownbag/Browbag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String\n",
    "\n",
    "A string type object is a sequence of characters. Each string is stored in computer array of characters. You can access individual characters by using indices in square\n",
    "brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "D\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "String= \"ABCD\"\n",
    "print(String[0])\n",
    "print(String[1])\n",
    "print(String[-1])\n",
    "print(String[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on string methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abcd. abcd.\n",
      "2\n",
      "False\n",
      "2\n",
      "['abcd', ' Abcd', '']\n"
     ]
    }
   ],
   "source": [
    "S4 = 'abcd. Abcd.'\n",
    "print(S4.capitalize())\n",
    "print(S4.count('c'))\n",
    "print(S4.isalnum())\n",
    "print(S4.index('c'))\n",
    "print(S4.split(sep='.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Write a function that counts the numbers of a particular letter in a string. For example count the number of letter a in a word **Bannana**. \n",
    "Note: Compare your function with a count method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def COUNT(Word, let):\n",
    "    word = Word\n",
    "    count = 0\n",
    "    for letter in word:\n",
    "        if letter == let:\n",
    "            count = count + 1\n",
    "    return print (count)\n",
    "\n",
    "COUNT('Bannana', 'a')\n",
    "# --------------------------------\n",
    "print('Bannana'.count('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use collections packages to create Counter Objects and  defaultdict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'c': 3, 'a': 2, 'b': 1, 'd': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list_char = ['a', 'b', 'c', 'd', 'a' , 'c', 'c']\n",
    "print(Counter(list_char))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "For example, let’s say you want the count of each name in a list of names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "defaultdict(<class 'int'>, {'John': 3, 'Julie': 1, 'Jack': 3, 'Ann': 1, 'Mike': 1, 'Jen': 3, 'Smith': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nums = defaultdict(int)\n",
    "nums['one'] = 1\n",
    "nums['two'] = 2\n",
    "print(nums['three'])\n",
    "\n",
    "count = defaultdict(int)\n",
    "names = \"John Julie Jack Ann Mike John John Jack Jack Jen Smith Jen Jen\"\n",
    "list1 = names.split(sep=' ')\n",
    "for names in list1:\n",
    "    count[names] +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Write a function that reads the Story text and finds the strings in the curly brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Story = \"\"\"\n",
    "Once upon a time, deep in an ancient jungle,there lived a {animal}. This {animal} liked to eat {food}, but the jungle had \n",
    "very little {food} to offer. One day, an explorer found the {animal} and discovered it liked {food}. The explorer took the\n",
    "{animal} back to {city}, where it could eat as much {food} as it wanted. However, the {animal} became homesick, so the\n",
    "explorer brought it back to the jungle, leaving a large supply of {food}.\n",
    "\n",
    "The End\n",
    "\"\"\"\n",
    "def getKeys(formatString):\n",
    "    ##\n",
    "    #To DO\n",
    "    ##\n",
    "    return keyList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle']\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle', ',', 'there', 'lived', 'a', '{', 'animal', '}', '.', 'This', '{', 'animal', '}', 'liked', 'to', 'eat', '{', 'food', '}', ',', 'but', 'the', 'jungle', 'had', 'very', 'little', '{', 'food', '}', 'to', 'offer', '.', 'One', 'day', ',', 'an', 'explorer', 'found', 'the', '{', 'animal', '}', 'and', 'discovered', 'it', 'liked', '{', 'food', '}', '.', 'The', 'explorer', 'took', 'the', '{', 'animal', '}', 'back', 'to', '{', 'city', '}', ',', 'where', 'it', 'could', 'eat', 'as', 'much', '{', 'food', '}', 'as', 'it', 'wanted', '.', 'However', ',', 'the', '{', 'animal', '}', 'became', 'homesick', ',', 'so', 'the', 'explorer', 'brought', 'it', 'back', 'to', 'the', 'jungle', ',', 'leaving', 'a', 'large', 'supply', 'of', '{', 'food', '}', '.', 'The', 'End']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import  nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(Story)\n",
    "print(type(tokens))\n",
    "print(tokens[:10])\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(120*'-')\n",
    "print( [lancaster.stem(t) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "tokens = word_tokenize(raw)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwprds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "0.5909126139346464\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "print(content_fraction(nltk.corpus.brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', 'would', 'get', 'home', '.']\n",
      "Counter({'the': 3, 'faster': 2, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, ',': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"The faster Harry got to the store, the faster Harry  would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(tokens)\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4), ('faster', 3), ('harry', 2), (',', 2)]\n",
      "0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"The faster Harry got to the store, the faster Harry \" \\\n",
    "           \"the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words.most_common(4))\n",
    "\n",
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "tf = times_harry_appears / num_unique_words\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "33\n",
      "18\n",
      "OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('is', 0), ('jill', 0), ('not', 0), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])\n",
      "          ,         .       and        as    faster       get       got  \\\n",
      "0  0.055556  0.055556  0.055556  0.000000  0.166667  0.055556  0.055556   \n",
      "1  0.000000  0.055556  0.055556  0.000000  0.055556  0.000000  0.000000   \n",
      "2  0.000000  0.055556  0.000000  0.111111  0.000000  0.000000  0.000000   \n",
      "\n",
      "      hairy     harry      home        is      jill       not     store  \\\n",
      "0  0.000000  0.111111  0.055556  0.000000  0.000000  0.000000  0.055556   \n",
      "1  0.055556  0.055556  0.000000  0.055556  0.055556  0.000000  0.000000   \n",
      "2  0.055556  0.055556  0.000000  0.055556  0.055556  0.055556  0.000000   \n",
      "\n",
      "       than       the        to     would  \n",
      "0  0.000000  0.166667  0.055556  0.055556  \n",
      "1  0.055556  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import OrderedDict, Counter\n",
    "import copy\n",
    "import pandas as pd\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "docs = [\"The faster Harry got to the store, the faster and faster \"\n",
    "        \"Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "print(len(doc_tokens[0]))\n",
    "all_doc_tokens = sum(doc_tokens, []) ; print(len(all_doc_tokens))\n",
    "lexicon = sorted(set(all_doc_tokens)); print(len(lexicon))\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon); print(zero_vector)\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "df = pd.DataFrame(doc_vectors);print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "data\n",
      "computers\n",
      "algorithms\n",
      "java\n",
      "programming\n",
      "structures\n",
      "learning\n",
      "machine\n",
      "science\n",
      "food\n",
      " \n",
      "Concept 1:\n",
      "fun\n",
      "family\n",
      "home\n",
      "kids\n",
      "money\n",
      "energy\n",
      "food\n",
      "games\n",
      "health\n",
      "love\n",
      " \n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Data Science Machine Learning\"\n",
    "doc2 = \"Money fun Family Kids home\"\n",
    "doc3 = \"Programming Java Data Structures\"\n",
    "doc4 = \"Love food health games energy fun\"\n",
    "doc5 = \"Algorithms Data Computers\"\n",
    "\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X =vectorizer.fit_transform(doc_complete)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2,n_iter=100)\n",
    "lsa.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logsitic Regression\n",
    "## Example 1:\n",
    "Read the data.csv file.\n",
    "Answer the following question\n",
    "\n",
    "1- In this dataset we have a lot of responses in text and each response has a label.\n",
    "\n",
    "2- Our goal is to correctly model the texts into its label.\n",
    "\n",
    "Hint: you need to read the text responses and perform preprocessing on it.\n",
    "such as normalization, legitimation, cleaning, stopwords removal and POS tagging.\n",
    "then use any methods you learned in the lecture to convert each response into meaningful numbers.\n",
    "\n",
    "3- Apply Naive bayes and look at appropriate evaluation metric.\n",
    "\n",
    "4- Explain your results very carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Amir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  80.5\n",
      "\n",
      "Naive Bayes Confusion Matrix -> \n",
      "  [[99 12]\n",
      " [27 62]]\n",
      "\n",
      "Naive Bayes Cohen Kappa Score ->  0.5985176034589252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "np.random.seed(500)\n",
    "\n",
    "Data = pd.read_csv(\"https://raw.githubusercontent.com/amir-jafari/NLP/master/Brownbag/data.csv\", encoding='latin-1').head(1000)\n",
    "Data['text'].dropna(inplace=True)\n",
    "Data['text'] = [entry.lower() for entry in Data['text']]\n",
    "Data['text'] = [word_tokenize(entry) for entry in Data['text']]\n",
    "\n",
    "tag_mapping = defaultdict(lambda: wn.NOUN)\n",
    "tag_mapping['J'] = wn.ADJ\n",
    "tag_mapping['V'] = wn.VERB\n",
    "tag_mapping['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(Data['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_mapping[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Data.loc[index, 'text_final'] = str(Final_words)\n",
    "    \n",
    "X_Train, X_Test, Y_Train, Y_Test = model_selection.train_test_split(Data['text_final'], Data['label'], test_size=0.2)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Y_Train = Encoder.fit_transform(Y_Train)\n",
    "Y_Test = Encoder.fit_transform(Y_Test)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=2000)\n",
    "Tfidf_vect.fit(Data['text_final'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_Train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_Test)\n",
    "\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Y_Train)\n",
    "\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(Y_Test, predictions_NB) * 100)\n",
    "print()\n",
    "print(\"Naive Bayes Confusion Matrix -> \\n \", confusion_matrix(Y_Test, predictions_NB))\n",
    "print()\n",
    "print(\"Naive Bayes Cohen Kappa Score -> \", cohen_kappa_score(Y_Test, predictions_NB))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Use logistic regression for to solve the Example 1 problem\n",
    "## Use LSA to impove your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "np.random.seed(500)\n",
    "\n",
    "Data = pd.read_csv(\"https://raw.githubusercontent.com/amir-jafari/NLP/master/Brownbag/data.csv\", encoding='latin-1').head(1000)\n",
    "##\n",
    "#TODO\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wise', 'young', 'pretty', 'boy', 'prince', 'princess', 'girl', 'king', 'man', 'woman', 'queen', 'strong'}\n",
      "king strong man\n",
      "queen wise woman\n",
      "boy young man\n",
      "girl young woman\n",
      "prince young king\n",
      "princess young queen\n",
      "man strong\n",
      "woman pretty\n",
      "prince boy king\n",
      "princess girl queen\n",
      "    input   label\n",
      "0    king  strong\n",
      "1    king     man\n",
      "2  strong    king\n",
      "3  strong     man\n",
      "4     man    king\n",
      "5     man  strong\n",
      "6   queen    wise\n",
      "7   queen   woman\n",
      "8    wise   queen\n",
      "9    wise   woman\n",
      "(52, 2)\n",
      "Epoch 0 | Loss 0.07827\n",
      "Epoch 1000 | Loss 0.06347\n",
      "Epoch 2000 | Loss 0.06329\n",
      "Epoch 3000 | Loss 0.06322\n",
      "Epoch 4000 | Loss 0.06317\n",
      "Epoch 5000 | Loss 0.06313\n",
      "Epoch 6000 | Loss 0.06310\n",
      "Epoch 7000 | Loss 0.06308\n",
      "Epoch 8000 | Loss 0.06306\n",
      "Epoch 9000 | Loss 0.06305\n",
      "Epoch 10000 | Loss 0.06304\n",
      "Epoch 11000 | Loss 0.06303\n",
      "Epoch 12000 | Loss 0.06302\n",
      "Epoch 13000 | Loss 0.06302\n",
      "Epoch 14000 | Loss 0.06301\n",
      "Epoch 15000 | Loss 0.06301\n",
      "Epoch 16000 | Loss 0.06300\n",
      "Epoch 17000 | Loss 0.06300\n",
      "Epoch 18000 | Loss 0.06299\n",
      "Epoch 19000 | Loss 0.06299\n",
      "[[ -3.4244251   -1.3984185 ]\n",
      " [ -3.5531247    1.7680206 ]\n",
      " [ -4.2644243   -0.41665173]\n",
      " [  7.637982    -9.043788  ]\n",
      " [ 35.33347      2.0485754 ]\n",
      " [ -1.4051887   -0.38903987]\n",
      " [ -3.5519884    1.727382  ]\n",
      " [  8.735041   -10.205565  ]\n",
      " [  9.594119   -10.040092  ]\n",
      " [ -3.2256835    1.5716752 ]\n",
      " [  1.8771387    6.089496  ]\n",
      " [ -3.5608299    1.8232226 ]]\n",
      "        word         x1         x2\n",
      "0       wise  -3.424425  -1.398419\n",
      "1      young  -3.553125   1.768021\n",
      "2     pretty  -4.264424  -0.416652\n",
      "3        boy   7.637982  -9.043788\n",
      "4     prince  35.333469   2.048575\n",
      "5   princess  -1.405189  -0.389040\n",
      "6       girl  -3.551988   1.727382\n",
      "7       king   8.735041 -10.205565\n",
      "8        man   9.594119 -10.040092\n",
      "9      woman  -3.225683   1.571675\n",
      "10     queen   1.877139   6.089496\n",
      "11    strong  -3.560830   1.823223\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy\n",
    "# -------------------------------------------------------------------------------------\n",
    "LR = 1e-2\n",
    "N_EPOCHS =20000\n",
    "PRINT_LOSS_EVERY = 1000\n",
    "EMBEDDING_DIM = 2\n",
    "# seed = 0\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# numpy.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "corpus = ['king is a strong man',\n",
    "          'queen is a wise woman',\n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong',\n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen']\n",
    "# -------------------------------------------------------------------------------------\n",
    "def remove_stop_words(corpus):\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(\" \".join(tmp))\n",
    "\n",
    "    return results\n",
    "# -------------------------------------------------------------------------------------\n",
    "corpus = remove_stop_words(corpus)\n",
    "# -------------------------------------------------------------------------------------\n",
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "words = set(words)\n",
    "print(words)\n",
    "# -------------------------------------------------------------------------------------\n",
    "word2int = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "\n",
    "sentences = []\n",
    "for sentence in corpus:\n",
    "    sentences.append(sentence.split())\n",
    "# -------------------------------------------------------------------------------------\n",
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0): min(idx + WINDOW_SIZE, len(sentence)) + 1]:\n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "# -------------------------------------------------------------------------------------\n",
    "for text in corpus:\n",
    "    print(text)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])\n",
    "print(df.head(10))\n",
    "print(df.shape)\n",
    "# -------------------------------------------------------------------------------------\n",
    "ONE_HOT_DIM = len(words)\n",
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding\n",
    "# -------------------------------------------------------------------------------------\n",
    "X = []\n",
    "Y = []\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "# -------------------------------------------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(12, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 12)\n",
    "        self.act1 = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        out_em = self.linear1(x)\n",
    "        output = self.linear2(out_em)\n",
    "        output = self.act1(output)\n",
    "        return out_em, output\n",
    "p = torch.Tensor(X_train)\n",
    "p.requires_grad = True\n",
    "t = torch.Tensor(Y_train)\n",
    "# %% -------------------------------------- Training Prep --------------------------------------------------------------\n",
    "model = MLP(EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "# %% -------------------------------------- Training Loop --------------------------------------------------------------\n",
    "for epoch in range(N_EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    _, t_pred = model(p)\n",
    "    loss = criterion(t, t_pred)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % PRINT_LOSS_EVERY == 0:\n",
    "        print(\"Epoch {} | Loss {:.5f}\".format(epoch, loss.item()))\n",
    "\n",
    "vectors = model.linear1._parameters['weight'].cpu().detach().numpy().transpose()\n",
    "\n",
    "# s1 = model.linear1._parameters['bias'].cpu().detach().numpy()[0] + model.linear1._parameters['weight'].cpu().detach()[0]\n",
    "# s2 = model.linear1._parameters['bias'].cpu().detach().numpy()[1] + model.linear1._parameters['weight'].cpu().detach()[1]\n",
    "# vvectors = torch.cat((s1,s2))\n",
    "# vectors = vvectors.reshape(12,2).cpu().detach().numpy()\n",
    "print(vectors)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = list(words)\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "print(w2v_df)\n",
    "# -------------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1, x2))\n",
    "\n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    "\n",
    "plt.xlim(x_axis_min, x_axis_max)\n",
    "plt.ylim(y_axis_min, y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sentiment Analysis With LSTM\n",
    "\n",
    "IWe will be building a machine learning model to detect sentiment using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. \n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "\n",
    "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n",
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [02:50, 5.05MB/s]                                                                    \n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:35<00:00, 11358.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  include_lengths = True)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "The next stage is building the model that we'll eventually train and evaluate. \n",
    "\n",
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
    "\n",
    "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
    "\n",
    "![](https://github.com/amir-jafari/NLP/blob/master/Brownbag/sentiment7.png?raw=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0447,  0.0882, -0.1086,  ..., -0.8002,  1.0247, -0.3785],\n",
      "        ...,\n",
      "        [ 1.3049,  1.6111,  1.6621,  ..., -0.9554, -2.8356, -0.6435],\n",
      "        [-1.1801, -0.7227, -1.8904,  ...,  0.9995,  0.2652,  0.5127],\n",
      "        [ 0.5679, -0.5493, -0.2391,  ...,  0.7662, -0.8018, -0.4563]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll set up the training and then train the model.\n",
    "\n",
    "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "## pipline\n",
    "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9718774557113647}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting to listen to the Brownbag session my whole life.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9839903712272644}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I've been waiting to listen to this terrible Brownbag session my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9992126822471619}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am not sure what I am doing but seems I am doing a heck of job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9985759258270264}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"How are you doing?.I am not too bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "\n",
    "We’ll start by tackling a more challenging task where we need to classify texts that haven’t been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the **zero-shot-classification** pipeline is very powerful: \n",
    "\n",
    "it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model. You’ve already seen how the model can classify a sentence as positive or negative using those two labels — but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71548e4300df4472861268149e2cf38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ac37243a56443a847334a20a95e29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852296d0d392406897b7567cddeda892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16e57b52c904557b875ff7c90db896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a030c979554f9f8460603e7d64e7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795dcddc331b481992700932f0626aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445967435836792, 0.1119757890701294, 0.04342745989561081]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This is a course about the Transformers library\", candidate_labels=[\"education\", \"politics\", \"business\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This brownbag session is about the machine learning and artifical inteligence',\n",
       " 'labels': ['data science', 'education', 'business', 'politics'],\n",
       " 'scores': [0.6876233220100403,\n",
       "  0.15481111407279968,\n",
       "  0.12108190357685089,\n",
       "  0.03648364171385765]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This brownbag session is about the machine learning and artifical inteligence\", \n",
    "           candidate_labels=[\"education\", \"politics\", \"business\", \"data science\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "Now let’s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it’s normal if you don’t get the same results as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016434999e3546e796ef39828c1ad301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ee23bd601e4a7fa50d4ea4e5b3af96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015a0436fcd048f985dc3b3535c66351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f6806fc56e422bb58bc069ee7044c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d2ae67504f48f0ace054cec2779d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop your own and share with others your skill, knowledge and skill in business, networking, marketing and technology. In order to practice this, your skills and insight required will vary.\\n\\nLearn more'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I am tired of listening to this brownbag session about natural language processing. I am tired of thinking about the question of why we don't have our minds trained on the fact that language learning cannot be developed in the classroom.\\n\\nFor me the answer is simple. We are not machines – nor are we talking about computers. Our brains have evolved and we have evolved to be computerized. If we are to get our brain to develop natural language processing or understand the language in a more natural way\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"I am tired of listening to this brownbag session about natural language processing.\",num_return_sequences = 1, max_length  = 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using any model from the Hub in a pipeline\n",
    "\n",
    "The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task — say, text generation.\n",
    "\n",
    "Go to the [Model Hub](https://huggingface.co/course/chapter1/3?fw=pt) and click on the corresponding tag on the left to display only the supported models for that task. \n",
    "\n",
    "Let’s try the distilgpt2 model! Here’s how to load it in the same pipeline as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c078e39eeb845ca8dbff8e687051505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249809f0e7f241af84419d8fce77fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d12011eef5497bacf4e1c8a45cbc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ce24b90aae4268b0eeb5c62b6695ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e40478fde3f4957aef79cda5a0076c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to make some use of both the new and the new techniques you want to use. By reading from these'},\n",
       " {'generated_text': 'In this course, we will teach you how to become a well-meaning individual who works for the community as a citizen.\\n\\n\\n\\n\\n'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this session, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask filling\n",
    "\n",
    "The next pipeline you’ll try is fill-mask. The idea of this task is to fill in the blanks in a given text:\n",
    "\n",
    "\n",
    "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special **\\<mask\\>** word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it’s always good to verify the proper mask word when exploring other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32307dfd5f249569f9739ffe07dd144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff002828c154c2e901a1e118f8b5083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb3dc57080b4c39a03e314097507ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f136c2a144398bf921abac96afa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74401456c6c944c48122927a4cfdff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This session will teach you all about mathematical models.',\n",
       "  'score': 0.1348658800125122,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This session will teach you all about computational models.',\n",
       "  'score': 0.040120597928762436,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This session will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I lost my life.',\n",
       "  'score': 0.18934619426727295,\n",
       "  'token': 685,\n",
       "  'token_str': ' lost'},\n",
       " {'sequence': 'I love my life.',\n",
       "  'score': 0.11497117578983307,\n",
       "  'token': 657,\n",
       "  'token_str': ' love'},\n",
       " {'sequence': 'I changed my life.',\n",
       "  'score': 0.09439051151275635,\n",
       "  'token': 1714,\n",
       "  'token_str': ' changed'},\n",
       " {'sequence': 'I hate my life.',\n",
       "  'score': 0.07370663434267044,\n",
       "  'token': 4157,\n",
       "  'token_str': ' hate'},\n",
       " {'sequence': 'I loved my life.',\n",
       "  'score': 0.041104670614004135,\n",
       "  'token': 2638,\n",
       "  'token_str': ' loved'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"I <mask> my life.\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. Let’s look at an example\n",
    "\n",
    "We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99919975,\n",
       "  'word': 'Amir',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99658424,\n",
       "  'word': 'CL',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9961011,\n",
       "  'word': 'District of Columbia',\n",
       "  'start': 36,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Amir and I work at CL in District of Columbia office.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering\n",
    "\n",
    "The question-answering pipeline answers questions using information from a given context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.43504536151885986,\n",
       " 'start': 30,\n",
       " 'end': 66,\n",
       " 'answer': 'CL in in District of Columbia office'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Amir and I work at CL in in District of Columbia office.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6118114590644836,\n",
       " 'start': 671,\n",
       " 'end': 683,\n",
       " 'answer': '€709 billion'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    #question=\"Where is the capital of France?\",\n",
    "    #question=\"What is population Paris?\",\n",
    "    question=\"What is GDP of Paris?\",\n",
    "    context=\"\"\"\n",
    "    Paris (French pronunciation: (About this soundlisten)) is the capital and most \n",
    "    populous city of France, with an estimated population of 2,175,601 residents as of 2018,\n",
    "    in an area of more than 105 square kilometres (41 square miles).[4] Since the 17th century,\n",
    "    Paris has been one of Europe's major centres of finance, diplomacy, commerce,\n",
    "    fashion, gastronomy, science, and arts. The City of Paris is the centre and seat\n",
    "    of government of the region and province of le-de-France, or Paris Region, \n",
    "    which has an estimated population of 12,174,880, or about 18 percent of the population\n",
    "    of France as of 2017.[5] The Paris Region had a GDP of €709 billion ($808 billion)\n",
    "    in 2017.[6] According to the Economist Intelligence Unit Worldwide Cost of Living Survey\n",
    "    in 2018, Paris was the second most expensive city in the world, after Singapore and ahead \n",
    "    of Zürich, Hong Kong, Oslo, and Geneva.[7] Another source ranked Paris as most expensive,\n",
    "    on a par with Singapore and Hong Kong, in 2018.[8][9]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8f92645c4c441a829008a7d4b8b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f99df95e9d4bbf96b082be2e86cd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7839837d484133bf87350ca4b75996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b68b50d3044b8f89e4655d641abfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331961935b0145318a171a4e6a6022ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\",max_length= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with a tokenizer\n",
    "\n",
    "Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
    "\n",
    "You can use Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow orsome models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It’s effectively a tensor; other ML frameworks’ tensors behave similarly, and are usually as simple to instantiate as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877edc6cdf474f1891bb3b6c686996c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d63ab91bcd4add8865a81ebe9e087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e8e25d56be4ec0be0b5847e708fe4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be840d9a8a41c6abadd424351e3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037,  2023,  2829,\n",
      "         16078,  5219,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'this', 'brown', '##bag', 'session', '.']\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = \"I've been waiting for a this brownbag session.\"\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(tokenizer.tokenize(raw_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, input_ids and attention_mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going through the model\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. Transformers provides an AutoModel class which also has a from_pretrained method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207211456e84fa5aee418c04f08d5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the **contextual understanding of that input by the Transformer model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the outputs of Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Head\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the AutoModel class, but AutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([[-0.1698,  0.1336]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.4247, 0.5753]], grad_fn=<SoftmaxBackward>)\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "raw_inputs = \"I've been waiting for a this brownbag session.\"\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Transformer\n",
    "\n",
    "The first thing we’ll need to do to initialize a BERT model is load a configuration object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different loading methods\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b062f4c9d0d41488c21f5f7614760a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ca9197c227420a85590317042205ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "The tokenization process is done by the tokenize method of the tokenizer.The conversion to input IDs is handled by the convert_tokens_to_ids tokenizer method.\n",
    "\n",
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode method as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\amir\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: dill in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: tqdm>=4.42 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (4.59.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (0.0.12)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\amir\\anaconda3\\lib\\site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\amir\\anaconda3\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\amir\\anaconda3\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Amir\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Loading cached processed dataset at C:\\Users\\Amir\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-2ff2f76fa35622cd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391487919b4045439d0b92e54470ae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Amir\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-83846288736966a5.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4319, grad_fn=<NllLossBackward>) torch.Size([8, 2])\n",
      "1377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbc9982e5b64fe1bee43136de8c1c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0e9e740bfa19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1523\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         )\n\u001b[1;32m--> 991\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    580\u001b[0m                 )\n\u001b[0;32m    581\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    511\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "print(num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "metric= load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
