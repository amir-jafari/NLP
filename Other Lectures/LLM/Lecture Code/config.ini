[env]
run_json_preprocessor = False

[instance]
user = aihan


[LMHeadModel]
model_name = gpt2
file_path = ${data:base_path}/Data/Amazon_Review_200.csv
model_path = ${data:base_path}/NLG_results/LMHeadModel/
top_p = 0.75
top_k = 3
temperature = 0.2
max_length = 128
distance_metric = cos
epochs = 1
batch_size = 4
save_steps = 500
colunm_name = review_body
out_file = ${data:base_path}/Data/Amazon_Review_LMHeadModel.csv

[casualLM]
model_name = gpt2
# bigscience/bloom-560m, bigscience/bloom-1b1, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, meta-llama/Llama-2-7b-chat-hf, EleutherAI/pythia-2.8b
file_path = ${data:base_path}/Data/Amazon_Review_200.csv
model_path = ${data:base_path}/NLG_results/causal/
top_p = 0.75
top_k = 3
temperature = 0.2
max_length = 128
distance_metric = cos
epochs = 1
save_steps = 500
colunm_name = review_body
out_file = ${data:base_path}/Data/Amazon_Review_casualLM.csv

[condi_gen]
model_name = t5-base
file_path = ${data:base_path}/Data/Amazon_Review_200_KW.csv
model_path = ${data:base_path}/NLG_results/T5_condigen/
top_p = 0.75
top_k = 3
temperature = 0.2
max_length = 128
distance_metric = cos
epochs = 1
batch_size = 4
save_steps = 2500
input_feature = keywords
label = review_body

[seq2seq]
model_name = t5-base
file_path = ${data:base_path}/Data/Amazon_Review_200_KW.csv
model_path = ${data:base_path}/NLG_results/T5_seq2seq/
top_p = 0.75
top_k = 3
temperature = 0.2
max_length = 128
distance_metric = cos
epochs = 1
batch_size = 16
save_steps = 500
input_feature = keywords
label = review_body


[casualLM_dolly]
model_name = facebook/opt-350m
# bigscience/bloom-560m, [bigscience/bloom-1b1, EleutherAI/gpt-neo-1.3B, meta-llama/Llama-2-7b-chat-hf, EleutherAI/pythia-2.8b](CUDA out of memory), tiiuae/falcon-7b-instruct (Process finished with exit code 137)
file_path = ${data:base_path}/Data/dolly-200.csv
model_path = ${data:base_path}/NLG_results/casualLM_dolly/
top_p = 0.75
top_k = 3
temperature = 0.2
test_size=0.3
max_length = 1024
distance_metric = cos
epochs = 1
batch_size = 1
learning_rate=1e-5
save_steps = 500
save_total_limit = 10
logging_steps = 10
input_feature = keywords
label = review_body

[casualLM_GTPQ]
model_name = TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ
# TheBloke/Llama-2-13B-GPTQ, TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ, TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ, roberta-base: # RuntimeError: The expanded size of the tensor (520) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 520].  Tensor sizes: [1, 514]
base_model = gptq_model-4bit-128g
file_path = ${data:base_path}/Data/dolly-200.csv
model_path = ${data:base_path}/NLG_results/casualLM_GTPQ/
top_p = 0.75
top_k = 3
temperature = 0.2
test_size=0.3
max_length = 1024
distance_metric = cos
epochs = 1
batch_size = 1
learning_rate=1e-5
save_steps = 500
save_total_limit = 10
logging_steps = 10
input_feature = keywords
label = review_body

[db]
vector_db = database
db_path = ${data:base_path}/db/${vector_db}/

[data]
base_path = /home/ubuntu/

[BedRock_LLM_API]
aws_access_key_id= None
aws_secret_access_key= None
aws_session_token= None